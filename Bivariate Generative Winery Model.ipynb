{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration with the Bivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I explore winery data with the two-dimensional Gaussian by varying the covariance matrix, drawing random samples from the resulting distribution, and plotting contour lines of the density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal \n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set.\n",
    "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
    "# Names of features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']\n",
    "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "trainx = data[perm[0:130],1:14]\n",
    "trainy = data[perm[0:130],0]\n",
    "testx = data[perm[130:178], 1:14]\n",
    "testy = data[perm[130:178],0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a gaussian to features\n",
    "def fit_gaussian(x,features):\n",
    "    mu = np.mean(x[:,features], axis=0)\n",
    "    covar=np.cov(x[:,features], rowvar=0, bias=1)\n",
    "    return mu, covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "[13.78534884  2.99627907]\n",
      "Covariance matrix:\n",
      "[[0.23325279 0.07526874]\n",
      " [0.07526874 0.15240941]]\n"
     ]
    }
   ],
   "source": [
    "f1=0\n",
    "f2=6\n",
    "label=1\n",
    "mu, covar = fit_gaussian(trainx[trainy==label], [f1,f2])\n",
    "print(\"Mean:\\n\" + str(mu))\n",
    "print(\"Covariance matrix:\\n\" + str(covar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find range of data\n",
    "def find_range(x):\n",
    "    lower = min(x)\n",
    "    upper = max(x)\n",
    "    width = upper - lower\n",
    "    lower = lower - 0.2 * width\n",
    "    upper = upper + 0.2 * width\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot density contours for 2 given classes\n",
    "def plot_contours(mu,covar,x1g,x2g,col):\n",
    "    rv = multivariate_normal(mean=mu,cov=covar)\n",
    "    z=np.zeros((len(x1g),len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            z[j,i]=rv.logpdf([x1g[i], x2g[j]])\n",
    "    sign, logdet = np.linalg.slogdet(covar)\n",
    "    normalizer = -0.5*(2*np.log(6.28)+sign*logdet)\n",
    "    for offset in range(1,4):\n",
    "        plt.contour(x1g,x2g,z, levels=[normalizer - offset], colors=col, linewidths=2.0, linestyles='solid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1025211be6f48908881dc8b42e960a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot density contours for 2 given classes. Also shows correlation of two features\n",
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1), label=IntSlider(1,1,3,1) )\n",
    "def two_features_plot(f1,f2,label):\n",
    "    if f1 == f2:\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return \n",
    "    \n",
    "    x1_lower, x1_upper =find_range(trainx[trainy==label,f1])\n",
    "    x2_lower, x2_upper =find_range(trainx[trainy==label,f2])\n",
    "    plt.xlim(x1_lower, x1_upper)\n",
    "    plt.ylim(x2_lower, x2_upper)\n",
    "    \n",
    "    res=200\n",
    "    x1g=np.linspace(x1_lower,x1_upper)\n",
    "    x2g=np.linspace(x2_lower,x2_upper)\n",
    "    \n",
    "    mu, covar = fit_gaussian(trainx[trainy==label], [f1,f2])\n",
    "    plot_contours(mu,covar,x1g,x2g,'k')\n",
    "    \n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Class ' + str(label), fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd0068fc3b4de58defd8ffa9b3ecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot density contours for all 3 classes. Also shows correlation of two features\n",
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def three_class_plot(f1,f2):\n",
    "    if f1 == f2:\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return\n",
    "    \n",
    "    x1_lower, x1_upper =find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper =find_range(trainx[:,f2])\n",
    "    plt.xlim(x1_lower, x1_upper)\n",
    "    plt.ylim(x2_lower, x2_upper)\n",
    "    \n",
    "    \n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "    \n",
    "    \n",
    "    res=200\n",
    "    x1g=np.linspace(x1_lower,x1_upper)\n",
    "    x2g=np.linspace(x2_lower,x2_upper)\n",
    "    \n",
    "    means=[]\n",
    "    covars=[]\n",
    "    \n",
    "    mu1, covar1 = fit_gaussian(trainx[trainy==1],[f1,f2])\n",
    "    mu2, covar2 = fit_gaussian(trainx[trainy==2],[f1,f2])\n",
    "    mu3, covar3 = fit_gaussian(trainx[trainy==3],[f1,f2])\n",
    "    \n",
    "    plot_contours(mu1,covar1,x1g,x2g,colors[0])    \n",
    "    plot_contours(mu2,covar2,x1g,x2g,colors[1])\n",
    "    plot_contours(mu3,covar3,x1g,x2g,colors[2])\n",
    "    \n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Wine data', fontsize=14, color='blue')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi=np.zeros((4,1))\n",
    "leny=len(trainy)\n",
    "for label in range(1,4):\n",
    "    pi[label] = len(trainy[trainy==label])/len(trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bivariate Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits bivariate generative model based on two selected features\n",
    "def fit_generative_model(x, y, features):\n",
    "    k = 3 # number of classes\n",
    "    d = len(features) # number of features\n",
    "    mu = np.zeros((k+1,d)) # list of means\n",
    "    covar = np.zeros((k+1,d,d)) # list of covariance matrices\n",
    "    pi = np.zeros(k+1) # list of class weights\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y==label)\n",
    "        mu[label,:], covar[label,:,:] = fit_gaussian(x[indices,:], features)\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, covar, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50980c5ec2fe4951b0736b26ac0a5dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests bivariate generative model and computes error of model\n",
    "@interact(f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1))\n",
    "\n",
    "def test_model(f1,f2):\n",
    "    if f1 == f2: # need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return\n",
    "    \n",
    "    features=[f1,f2]\n",
    "\n",
    "    mu, covar, pi=fit_generative_model(trainx,trainy,[f1,f2])\n",
    "    \n",
    "    k=3\n",
    "    leny=len(testx)\n",
    "    score=np.zeros((leny,k+1))\n",
    "    \n",
    "    for i in range(0,leny):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label]=np.log(pi[label]) + \\\n",
    "            multivariate_normal.logpdf(testx[i, features], mean=mu[label,:], cov=covar[label,:,:])\n",
    "    predictions=np.argmax(score[:,1:4], axis=1)+1\n",
    "        \n",
    "    errors = np.sum(predictions!=testy)\n",
    "    print(\"Test error using feature(s): \"),\n",
    "    for f in features:\n",
    "        print(\"'\" + featurenames[f] + \"'\" + \" \"),\n",
    "    print\n",
    "    print(\"Errors: \" + str(errors) + \"/\" + str(leny))\n",
    "    \n",
    "    percent_error=errors/leny*100\n",
    "    print(percent_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Decision Boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38f159e68bf410b9aa47c01209e67bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shows bivariate generative model and decision boundaries\n",
    "@interact(f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1))\n",
    "def show_decision_boundary(f1,f2):\n",
    "    \n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
    "    \n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim([x1_lower,x1_upper])\n",
    "    plt.ylim([x2_lower,x2_upper])\n",
    "\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "    \n",
    "    random_vars = {}\n",
    "    for label in range(1,4):\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
    "\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            scores = []\n",
    "            for label in range(1,4):\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) + 1\n",
    "            \n",
    "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
    "    \n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
